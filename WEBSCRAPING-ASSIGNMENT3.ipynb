{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2571c733",
   "metadata": {},
   "source": [
    "# WEBSCRAPING-ASSIGNMENT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61548445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.1 & 2 - Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search.click()\n",
    "\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url[0:2]:\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "    next=driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "    next.click()\n",
    "    time.sleep(5)   \n",
    "\n",
    "\n",
    "#Brandname\n",
    "#Name of the product\n",
    "#price\n",
    "#return/exchange\n",
    "#expected delivery\n",
    "# Availabilit\n",
    "# produt url\n",
    "\n",
    "Brand=[]\n",
    "Product=[]\n",
    "Price=[]\n",
    "rox=[]\n",
    "ExpectedDelivery=[]\n",
    "Availability=[]\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'//td[@class=\"a-span9\"]')\n",
    "        Brand.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"-\")\n",
    "Brand\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        product=driver.find_element(By.XPATH,'//span[@class=\"a-size-large product-title-word-break\"]')\n",
    "        Product.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        Product.append(\"-\")\n",
    "Product\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append(\"-\")\n",
    "Price\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        exchange=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[4]/div[21]/div/div[1]/div[2]/div/div/div/div[3]/span/div[2]')\n",
    "        rox.append(exchange.text)\n",
    "    except NoSuchElementException:\n",
    "        rox.append(\"-\")\n",
    "rox\n",
    "\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        edd=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[1]/div[3]/div/div/div[1]/div/div/div[1]/div/div[2]/div[12]/div[1]/div/div/div[3]/span/span[1]')\n",
    "        ExpectedDelivery.append(edd.text)\n",
    "    except NoSuchElementException:\n",
    "        ExpectedDelivery.append(\"-\")\n",
    "ExpectedDelivery\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        availa=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[1]/div[3]/div/div/div/div/form/div/div/div/div/div[3]/div/div[4]/div')\n",
    "        Availability.append(availa.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append(\"-\")\n",
    "Availability\n",
    "\n",
    "print(len(Brand),len(Product),len(Price),len(rox),len(ExpectedDelivery),len(Availability))\n",
    "\n",
    "df=pd.DataFrame({\"Product\":Product,\"Brand\":Brand,\"Price\":Price,\"Avaialability\":Availability,\"Return/Exchange\":rox,\"Expected delivery date\":ExpectedDelivery})\n",
    "df\n",
    "\n",
    "df.to_csv('amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.3 Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://images.google.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "product.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div\")\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls=[]\n",
    "\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "img_urls[0:10]\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    #if i>=10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\Win\\Desktop\\Assignments\\images\\images1\" +str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.4 Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "cross=driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "cross.click()\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "product.send_keys('smartphone')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "#“Brand Name”, \n",
    "#“Smartphone name”, \n",
    "#“Colour”, \n",
    "#“RAM”, \n",
    "#“Storage(ROM)”, \n",
    "#“Primary Camera”, \n",
    "#“Secondary Camera”, \n",
    "#“Display Size”, \n",
    "#“Battery Capacity”, \n",
    "#“Price”, \n",
    "#“Product URL”. \n",
    "#Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n",
    "\n",
    "urls = []\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "# getting the required data inside the empty lists\n",
    "brand_names = []\n",
    "name = []\n",
    "color = []\n",
    "RAM = []\n",
    "storage = []\n",
    "prim_cam = []\n",
    "sec_cam = []\n",
    "Display_size = []\n",
    "display_resolution = []\n",
    "processor = []\n",
    "processor_core = []\n",
    "battery_cap = []\n",
    "prices = []\n",
    "product_url = []\n",
    "\n",
    "# fetching battery capacity\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='fMghEO']\"):\n",
    "    try:\n",
    "        bat_cap = driver.find_element_by_xpath(\"//ul[@class='_1xgFaf']//li[4]\")\n",
    "        battery_cap.append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        battery_cap.append('-')\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # fetching brand names\n",
    "    try:\n",
    "        br_name = driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/div[4]/a\")\n",
    "        brand_names.append(br_name.text.replace('Mobiles',''))\n",
    "    except NoSuchElementException:\n",
    "        brand_names.append('-')\n",
    "    # fetching the smartphone names\n",
    "    try:\n",
    "        smt_name = driver.find_element_by_xpath(\"//h1[@class='yhB1nd']//span\")\n",
    "        name.append(smt_name.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append('-')\n",
    "    \n",
    "    # fetching colors of smartphone\n",
    "    try:\n",
    "        clr = driver.find_element_by_xpath(\"//table[@class='_14cfVK']//tr[4]//td[2]\")\n",
    "        color.append(clr.text)\n",
    "    except NoSuchElementException:\n",
    "        color.append('-')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # getting the read more button\n",
    "    try:\n",
    "        read_more_btn = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\").click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # fetching the display size\n",
    "    try:\n",
    "        disp_size = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[2]//tr[1]//td[2]\")\n",
    "        Display_size.append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_size.append\n",
    "        \n",
    "    # fetching the display resolution\n",
    "    try:\n",
    "        disp_res = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[2]//tr[2]//td[2]\")\n",
    "        display_resolution.append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        display_resolution.append('-')\n",
    "        \n",
    "    # fetching the processor information\n",
    "    try:\n",
    "        pro = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[3]//tr[2]//td[2]\")\n",
    "        processor.append(pro.text)\n",
    "    except NoSuchElementException:\n",
    "        processor.append('-')\n",
    "        \n",
    "    # fetching the processor and number of core details\n",
    "    try:\n",
    "        pro_core = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[3]//tr[3]//td[2]\")\n",
    "        processor_core.append(pro_core.text)\n",
    "    except NoSuchElementException:\n",
    "        processor_core.append('-')\n",
    "        \n",
    "    # fetching the storage/ROM details\n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[4]//tr[1]//td[2]\")\n",
    "        storage.append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        storage.append('-')\n",
    "        \n",
    "    # fetching the RAM information\n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[4]//tr[2]//td[2]\")\n",
    "        RAM.append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # fetching the primary camera information\n",
    "    try:\n",
    "        p_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[5]//tr[2]//td[2]\")\n",
    "        prim_cam.append(p_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        prim_cam.append('-')\n",
    "        \n",
    "    # fetching the secondary camera information\n",
    "    try:\n",
    "        s_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[5]//tr[5]//td[2]\")\n",
    "        sec_cam.append(s_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        sec_cam.append('-')        \n",
    "   \n",
    "    # fetching the price of the smartphone\n",
    "    try:\n",
    "        price = driver.find_element_by_xpath(\"//div[@class='_30jeq3 _16Jk6d']\")\n",
    "        prices.append(price.text.replace('₹','Rs. '))\n",
    "    except NoSuchElementException:\n",
    "        prices.append('-')\n",
    "    \n",
    "    \n",
    "# creating a dataframe now with the obtainined details\n",
    "data = list(zip(brand_names,name,color,RAM,storage,prim_cam,sec_cam,Display_size,display_resolution,processor,\n",
    "                processor_core,battery_cap,prices,urls))\n",
    "df = pd.DataFrame(data, columns = [\"Brand Name\",\"Smartphone Name\",\"Color\",\"RAM\",\"Storage/ROM\",\"Primary Camera\",\n",
    "                                   \"Secondary Camera\",\"Display Size\",\"Display Resolution\",\"Processor\",\"Processor-Core\",\n",
    "                                   \"Battery Capacity\",\"Price\",\"Product Url\"])\n",
    "df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.5 - Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "driver.maximize_window()\n",
    "\n",
    "place=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div/div[2]/form/input[1]\")\n",
    "place.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div/div[2]/div[1]/button\")\n",
    "search.click()\n",
    "\n",
    "url_string = driver.current_url\n",
    "print(url_string)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "try:\n",
    "    lat = re.findall('(\\d+(\\.\\d*)?)', current_url)[0]   # fetching the latitude data\n",
    "    lon = re.findall('(\\d+(\\.\\d*)?)', current_url)[1]  # fetching the longitude data\n",
    "    print(\"Geospatial Coordinates for\", place + \" on Google Maps are: \")\n",
    "    print(\"Latitude  :\", lat[0])\n",
    "    print(\"Longitude :\", lon[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error: \", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.6 - Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in.\n",
    "\n",
    "# Q.No.7 - Write a program to scrap all the available details of best gaming laptops from digit.in.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "top=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[4]/ul/li[4]/a')\n",
    "top.click()\n",
    "\n",
    "laptop=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[5]/div[1]/div/button[2]')\n",
    "laptop.click()\n",
    "time.sleep(3)\n",
    "\n",
    "gaming=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[5]/div[3]/div[3]/a/div[2]')\n",
    "gaming.click()\n",
    "\n",
    "\n",
    "# creating empty lists to store all gaming laptop information\n",
    "lap_name = []\n",
    "op_stm = []\n",
    "display = []\n",
    "processor = []\n",
    "memory = []\n",
    "weight = []\n",
    "dimensions = []\n",
    "graph_processor = []\n",
    "prices = []\n",
    "\n",
    "# fetching the laptop names\n",
    "name = driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[1]\")\n",
    "for i in name:\n",
    "    lap_name.append(i.text)\n",
    "    \n",
    "# fetching information related to operating system\n",
    "try:\n",
    "    os = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[3]//td[3]\")\n",
    "    for i in os:\n",
    "        op_stm.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for display\n",
    "try:\n",
    "    disp = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[4]//td[3]\")\n",
    "    for i in disp:\n",
    "        display.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for processor\n",
    "try:\n",
    "    pro = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[5]//td[3]\")\n",
    "    for i in pro:\n",
    "        processor.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for memory\n",
    "try:\n",
    "    mem = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[6]//td[3]\")\n",
    "    for i in mem:\n",
    "        memory.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for weight\n",
    "try:\n",
    "    wgt = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[7]//td[3]\")\n",
    "    for i in wgt:\n",
    "        weight.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for dimensions\n",
    "try:\n",
    "    dim = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[8]//td[3]\")\n",
    "    for i in dim:\n",
    "        dimensions.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for graphic processor\n",
    "try:\n",
    "    g_pro = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[9]//td[3]\")\n",
    "    for i in g_pro:\n",
    "        graph_processor.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching information for price\n",
    "try:\n",
    "    price = driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "    for i in price:\n",
    "        prices.append(i.text.replace('₹','Rs '))\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# now creating a dataframe from all the collected details\n",
    "data = list(zip(lap_name,op_stm,display,processor,memory,weight,dimensions,graph_processor,prices))\n",
    "df = pd.DataFrame(data, columns = [\"Laptop Name\",\"Operating System\",\"Display\",\"Processor\",\"Memory\",\n",
    "                                   \"Weight(Kg)\",\"Dimensions\",\"Graphic-Processor\",\"Price\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620366a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.8 - Write a python program to scrape the details for all billionaires from www.forbes.com.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.forbes.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[1]')\n",
    "search.click()\n",
    "\n",
    "billioniare=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]')\n",
    "billioniare.click()\n",
    "time.sleep(10)\n",
    "\n",
    "wb=driver.find_elements(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]/div[2]/div[3]/ul/li[1]/a')\n",
    "wb.click()\n",
    "\n",
    "\n",
    "# fetching the rank\n",
    "rank = []\n",
    "try:\n",
    "    rnk = driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for i in rnk:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching the names of the billionaires\n",
    "names = []\n",
    "try:\n",
    "    name = driver.find_elements_by_xpath(\"//div[@class='personName']\")\n",
    "    for i in name:\n",
    "        names.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching the total net worth\n",
    "net_worth = []\n",
    "try:\n",
    "    worth = driver.find_elements_by_xpath(\"//div[@class='netWorth']\")\n",
    "    for i in worth:\n",
    "        net_worth.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# fetching the age of the billionaires\n",
    "Age = []\n",
    "try:\n",
    "    age = driver.find_elements_by_xpath(\"//div[@class='age']\")\n",
    "    for i in age:\n",
    "        Age.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Age.append('-')\n",
    "    \n",
    "# fetching the citizenship of the billionaires\n",
    "citizenship = []\n",
    "try:\n",
    "    cit = driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for i in cit:\n",
    "        citizenship.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    citizenship.append('-')\n",
    "    \n",
    "# fetching the source of incomes for the billionaires\n",
    "source = []\n",
    "try:\n",
    "    src = driver.find_elements_by_xpath(\"//div[@class='source-column']\")\n",
    "    for i in src:\n",
    "        source.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    source.append('-')\n",
    "    \n",
    "# fetching the industry in which the billionaires are prominent\n",
    "industry = []\n",
    "try:\n",
    "    ind = driver.find_elements_by_xpath(\"//div[@class='category']\")\n",
    "    for i in ind:\n",
    "        industry.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    industry.append('-')\n",
    "    \n",
    "# now creating a dataframe from all the collected information\n",
    "data = list(zip(rank,names,net_worth,Age,citizenship,source,industry))\n",
    "df = pd.DataFrame(data, columns = [\"Rank\", \"Name\", \"Net worth\", \"Age\", \"Citizenship\", \"Source\",\"Industry\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5520ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.9 - Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#x=input(\"Enter the product to search: \")\n",
    "\n",
    "#print(x)\n",
    "\n",
    "driver.get(\"https://www.youtube.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/form/div[1]/div[1]/input\")\n",
    "product.send_keys('Python programming')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/button')\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "\n",
    "video=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div[2]/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-video-renderer[2]/div[1]/div/div[1]/div/h3/a/yt-formatted-string')\n",
    "video.click()\n",
    "\n",
    "comments=[]\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "comments_tags=driver.find_elements(By.XPATH,'//div[@class=\"style-scope ytd-expander\"]')\n",
    "for i in comments_tags[0:500]:\n",
    "    comment=i.text\n",
    "    comments.append(comment)\n",
    "comments\n",
    "\n",
    "time=[]\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "period_tags=driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]')\n",
    "for i in period_tags[0:500]:\n",
    "    period=i.text\n",
    "    time.append(period)\n",
    "time\n",
    "\n",
    "print(len(comments),len(time))\n",
    "\n",
    "df=pd.DataFrame({\"Comments\":comments,\"Period\":time})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f693952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.10 - Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "time.sleep(5)\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "location_tags=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/input')\n",
    "location_tags.send_keys(\"London\")\n",
    "time.sleep(3)\n",
    "\n",
    "button = driver.find_element_by_xpath(\"//div[@class='label']\")\n",
    "button.click()\n",
    "\n",
    "search_button = driver.find_element_by_id('search-button')\n",
    "search_button.click()\n",
    "\n",
    "# creating empty lists to collect the required information\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    # fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "    # fetching the distance from city centre    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        \n",
    "    # fetching the privates from price details\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "            \n",
    "    # fetching the dorms from price information\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "    # fetching all the facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "        \n",
    "    # fetching the urls of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # clicking on the show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    # fetching the ratings for the hostels\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "    # fetching the total reviews        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        \n",
    "    # fetching the overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "        \n",
    "    # fetching the property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "\n",
    "# now creating a pandas dataframe to show all the collected data\n",
    "data = list(zip(hostel_name,distance,rating,reviews,over_all,pvt_prices,dorms_price,facilities,description))       \n",
    "df = pd.DataFrame(data, columns = [\"Hostel name\",\"Distance from city centre\",\"ratings\",\"Total reviews\",\n",
    "                                   \"Overall review\",\"Privates from price\",\"Dorms from price\",\"Facilities\",\"Property Description\"])\n",
    "df\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
