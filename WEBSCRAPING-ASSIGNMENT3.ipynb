{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2571c733",
   "metadata": {},
   "source": [
    "# WEBSCRAPING-ASSIGNMENT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61548445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.1 & 2 - Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "search.click()\n",
    "\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url[0:2]:\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "    next=driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "    next.click()\n",
    "    time.sleep(5)   \n",
    "\n",
    "\n",
    "#Brandname\n",
    "#Name of the product\n",
    "#price\n",
    "#return/exchange\n",
    "#expected delivery\n",
    "# Availabilit\n",
    "# produt url\n",
    "\n",
    "Brand=[]\n",
    "Product=[]\n",
    "Price=[]\n",
    "rox=[]\n",
    "ExpectedDelivery=[]\n",
    "Availability=[]\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'//td[@class=\"a-span9\"]')\n",
    "        Brand.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"-\")\n",
    "Brand\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        product=driver.find_element(By.XPATH,'//span[@class=\"a-size-large product-title-word-break\"]')\n",
    "        Product.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        Product.append(\"-\")\n",
    "Product\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append(\"-\")\n",
    "Price\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        exchange=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[4]/div[21]/div/div[1]/div[2]/div/div/div/div[3]/span/div[2]')\n",
    "        rox.append(exchange.text)\n",
    "    except NoSuchElementException:\n",
    "        rox.append(\"-\")\n",
    "rox\n",
    "\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        edd=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[1]/div[3]/div/div/div[1]/div/div/div[1]/div/div[2]/div[12]/div[1]/div/div/div[3]/span/span[1]')\n",
    "        ExpectedDelivery.append(edd.text)\n",
    "    except NoSuchElementException:\n",
    "        ExpectedDelivery.append(\"-\")\n",
    "ExpectedDelivery\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        availa=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[5]/div[4]/div[1]/div[3]/div/div/div/div/form/div/div/div/div/div[3]/div/div[4]/div')\n",
    "        Availability.append(availa.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append(\"-\")\n",
    "Availability\n",
    "\n",
    "print(len(Brand),len(Product),len(Price),len(rox),len(ExpectedDelivery),len(Availability))\n",
    "\n",
    "df=pd.DataFrame({\"Product\":Product,\"Brand\":Brand,\"Price\":Price,\"Avaialability\":Availability,\"Return/Exchange\":rox,\"Expected delivery date\":ExpectedDelivery})\n",
    "df\n",
    "\n",
    "df.to_csv('amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.3 Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://images.google.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input\")\n",
    "product.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div\")\n",
    "search.click()\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls=[]\n",
    "\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "img_urls[0:10]\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    #if i>10:\n",
    "        #breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\Win\\Desktop\\Assignments\\images\\images1\" +str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.4 Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "cross=driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/button\")\n",
    "cross.click()\n",
    "driver.maximize_window()\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "product.send_keys('smartphone')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "#“Brand Name”, \n",
    "#“Smartphone name”, \n",
    "#“Colour”, \n",
    "#“RAM”, \n",
    "#“Storage(ROM)”, \n",
    "#“Primary Camera”, \n",
    "#“Secondary Camera”, \n",
    "#“Display Size”, \n",
    "#“Battery Capacity”, \n",
    "#“Price”, \n",
    "#“Product URL”. \n",
    "#Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n",
    "\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "    for i in url[0:10]:\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "product_urls\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.5 - Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "x=input(\"Enter the product to search: \")\n",
    "\n",
    "print(x)\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "driver.maximize_window()\n",
    "\n",
    "place=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div/div[2]/form/input[1]\")\n",
    "place.send_keys(x)\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div/div[2]/div[1]/button\")\n",
    "search.click()\n",
    "\n",
    "url_string = driver.current_url\n",
    "print(url_string)\n",
    "\n",
    "\n",
    "m=url_string.split('@')\n",
    "m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.6 - Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in.\n",
    "\n",
    "# Q.No.7 - Write a program to scrap all the available details of best gaming laptops from digit.in.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "top=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[4]/ul/li[4]/a')\n",
    "top.click()\n",
    "\n",
    "laptop=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[5]/div[1]/div/button[2]')\n",
    "laptop.click()\n",
    "time.sleep(3)\n",
    "\n",
    "gaming=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[5]/div[3]/div[3]/a/div[2]')\n",
    "gaming.click()\n",
    "\n",
    "list=[]\n",
    "\n",
    "list_tags=driver.find_elements(By.XPATH,'//div[@class=\"TopNumbeHeading\"]')\n",
    "for i in list_tags:    \n",
    "    list1=i.text\n",
    "    list.append(list1)\n",
    "list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620366a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.8 - Write a python program to scrape the details for all billionaires from www.forbes.com.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.forbes.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[1]')\n",
    "search.click()\n",
    "\n",
    "billioniare=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]')\n",
    "billioniare.click()\n",
    "time.sleep(10)\n",
    "\n",
    "wb=driver.find_elements(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]/div[2]/div[3]/ul/li[1]/a')\n",
    "wb.click()\n",
    "\n",
    "\n",
    "name=[]\n",
    "name_tags=driver.find_elements(By.XPATH,'//div[@class=\"personName\"]')\n",
    "for i in name_tags:\n",
    "    name1=i.text\n",
    "    name.append(name1)\n",
    "name\n",
    "\n",
    "rank=[]\n",
    "rank_tags=driver.find_elements(By.XPATH,'//div[@class=\"rank\"]')\n",
    "for i in rank_tags:\n",
    "    rank1=i.text\n",
    "    rank.append(rank1)\n",
    "rank\n",
    "\n",
    "net=[]\n",
    "net_tags=driver.find_elements(By.XPATH,'//div[@class=\"netWorth\"]')\n",
    "for i in net_tags:\n",
    "    net1=i.text\n",
    "    net.append(net1)\n",
    "net\n",
    "\n",
    "age=[]\n",
    "age_tags=driver.find_elements(By.XPATH,'//div[@class=\"age\"]')\n",
    "for i in age_tags:\n",
    "    age1=i.text\n",
    "    age.append(age1)\n",
    "age\n",
    "\n",
    "citiz=[]\n",
    "citiz_tags=driver.find_elements(By.XPATH,'//div[@class=\"countryOfCitizenship\"]')\n",
    "for i in citiz_tags:\n",
    "    citiz1=i.text\n",
    "    citiz.append(citiz1)\n",
    "citiz\n",
    "\n",
    "source=[]\n",
    "source_tags=driver.find_elements(By.XPATH,'//div[@class=\"source-column\"]')\n",
    "for i in source_tags:\n",
    "    source1=i.text\n",
    "    source.append(source1)\n",
    "source\n",
    "\n",
    "category=[]\n",
    "category_tags=driver.find_elements(By.XPATH,'//div[@class=\"category\"]')\n",
    "for i in category_tags:\n",
    "    category1=i.text\n",
    "    category.append(category1)\n",
    "category\n",
    "\n",
    "print(len(rank),len(name),len(net),len(age),len(citiz),len(source),len(category))\n",
    "\n",
    "df=pd.DataFrame({\"Rank\":rank,\"Name\":name,\"Networth\":net,\"Age\":age,\"Citizenship\":citiz,\"Source\":source,\"Industry\":category})\n",
    "df\n",
    "\n",
    "#Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5520ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.No.9 - Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#x=input(\"Enter the product to search: \")\n",
    "\n",
    "#print(x)\n",
    "\n",
    "driver.get(\"https://www.youtube.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "product=driver.find_element(By.XPATH,\"/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/form/div[1]/div[1]/input\")\n",
    "product.send_keys('Python programming')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/button')\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "\n",
    "video=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div[2]/div/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-video-renderer[2]/div[1]/div/div[1]/div/h3/a/yt-formatted-string')\n",
    "video.click()\n",
    "\n",
    "comments=[]\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "comments_tags=driver.find_elements(By.XPATH,'//div[@class=\"style-scope ytd-expander\"]')\n",
    "for i in comments_tags[0:500]:\n",
    "    comment=i.text\n",
    "    comments.append(comment)\n",
    "comments\n",
    "\n",
    "time=[]\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "period_tags=driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]')\n",
    "for i in period_tags[0:500]:\n",
    "    period=i.text\n",
    "    time.append(period)\n",
    "time\n",
    "\n",
    "print(len(comments),len(time))\n",
    "\n",
    "df=pd.DataFrame({\"Comments\":comments,\"Period\":time})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f693952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.No.10 - Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Win\\Desktop\\chromedriver_win32\\chromedriver.exe\")\n",
    "time.sleep(5)\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "location_tags=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/input')\n",
    "location_tags.send_keys(\"London\")\n",
    "time.sleep(3)\n",
    "\n",
    "select=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "select.send_keys(\"London\")\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
